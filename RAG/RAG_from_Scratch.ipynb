{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries to install\n",
    "# !pip install -U sentence-transformers\n",
    "# !pip install wikipedia-api\n",
    "# !pip install accelerate\n",
    "# !pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wikipediaapi import Wikipedia\n",
    "import numpy as np\n",
    "\n",
    "# Grab text from wikipedia page.\n",
    "wiki = Wikipedia(\"RAGBot/0.0\", \"en\")\n",
    "\n",
    "page = wiki.page(\"Pokemon\")\n",
    "print(page.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_chunks = page.text.split(\"\\n\\n\") # Split wall of text into chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chunk overlapping. Ensures context is retained across chunks.\n",
    "overlap_amount = 10\n",
    "for i in range(1, len(text_chunks)):\n",
    "    prev_string = text_chunks[i-1].split()\n",
    "    overlap_words = np.array(prev_string)[len(prev_string)-overlap_amount::]\n",
    "    text_chunks[i] = \" \".join(overlap_words) + \" \" + text_chunks[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_chunks[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sentence_encoder = SentenceTransformer(\"BAAI/bge-small-en-v1.5\") # Load text encoder model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks_embedding = sentence_encoder.encode(text_chunks, normalize_embeddings=True) # Convert text into ML understandable format. Normalize data so we can easily use dot product to find similarities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_input = \"What was Ash's first pokemon?\"\n",
    "input_embedding = sentence_encoder.encode(user_input, normalize_embeddings=True) # Encode user input, normalize for dot product use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarities = np.dot(chunks_embedding, input_embedding.T) # Find the similarities between the two."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "top_k_probabiliies = np.sort(similarities)[::-1][::k] # Grab the top k highest similarity probability, just for us to see :^)\n",
    "top_k_probabiliies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_indices = np.argsort(similarities)[::-1][::k] # Grab the indices of the top k entries.\n",
    "top_k_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_k_paragraphs = np.array(text_chunks)[top_k_indices] # Grab the top k most similar paragraphs to our query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text in top_k_paragraphs:\n",
    "    print(\"----------------------------------------------------------------\\n\")\n",
    "    print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"Qwen/Qwen2-1.5B\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\") # Load LLM\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast = True) # Load tokenizer correlating to LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the instruction, context, and user input string for the model.\n",
    "context_string = \"Context:\\n\\n\"\n",
    "for paragraph in top_k_paragraphs:\n",
    "    context_string = context_string + paragraph + \"\\n\\n\"\n",
    "\n",
    "instruction_strings = f\"\"\"\n",
    "You are a personal assistant who specializes in a wide variety of topics. Communicate in a clear, accessible language, escalating to technical depth upon request.\n",
    "  React to feedback aptly and end responses with '-Yours Truly'.\n",
    "\n",
    "  You will tailor the length of its responses to match the viewer's comment, providing concise acknowledgments to brief expressions of gratitude or feedback,\n",
    "thus keeping the interaction natural and engaging.\n",
    "\"\"\"\n",
    "\n",
    "prompt = lambda context, user_input: f\"\"\"\n",
    "[INST]\n",
    "{instruction_strings}\n",
    "\n",
    "{context}\n",
    "\n",
    "Please answer the following comment. Use the context above if it helps.\n",
    "\n",
    "{user_input}\n",
    "[\\INST]\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "model_input = prompt(context_string, \"Who was Ash's first Pokemon?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(model_input, return_tensors=\"pt\")\n",
    "result = model.generate(input_ids = tokens[\"input_ids\"], max_new_tokens = 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragENV",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
